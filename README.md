# å¤§æ¨¡å‹é‡åŒ–å·¥å…· (LLM Quantization Toolkit)

ä¸€ä¸ªçµæ´»ã€å¯æ‰©å±•çš„å¤§æ¨¡å‹é‡åŒ–å·¥å…·ï¼Œæ”¯æŒå¤šç§é‡åŒ–ç®—æ³•å’Œæ¨¡å‹æ¶æ„ï¼Œ**å¯ç›´æ¥åœ¨vLLMä¸Šéƒ¨ç½²**ã€‚

## ğŸ“– æ–‡æ¡£å¯¼èˆª

- **[ğŸš€ å¿«é€Ÿä¸Šæ‰‹æŒ‡å—](docs/quick_start.md)** - 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹
- **[ğŸ—ï¸ 4+1æ¶æ„è§†å›¾](docs/4plus1_architecture.md)** - ç³»ç»Ÿæ¶æ„è¯¦è§£
- **[ğŸ¯ è®¾è®¡ç†å¿µè¯´æ˜](docs/design_philosophy.md)** - æ·±å…¥ç†è§£è®¾è®¡å†³ç­–
- **[ğŸ“Š Mermaidå›¾è¡¨](docs/mermaid_diagrams.md)** - å¯è§†åŒ–æ¶æ„å›¾
- **[ğŸ”§ æ‰©å±•å¼€å‘æŒ‡å—](docs/extension_guide.md)** - æ·»åŠ æ–°ç®—æ³•å’Œæ¨¡å‹

## ç‰¹æ€§

- ğŸš€ **å¤šæ¨¡å‹æ”¯æŒ**: Llama, Qwen, DeepSeek, ç­‰ä¸»æµå¤§æ¨¡å‹
- ğŸ”§ **å¤šé‡åŒ–ç®—æ³•**: GPTQ, AWQ, MinMax, HQQ, ç­‰å…ˆè¿›é‡åŒ–æ–¹æ³•
- ğŸ¯ **ç¦»ç¾¤å€¼æŠ‘åˆ¶**: SmoothQuant, ç­‰ç®—æ³•æ”¯æŒ
- ğŸ“Š **é€å±‚é‡åŒ–**: æ”¯æŒæŒ‰å±‚è¿›è¡Œé‡åŒ–
- ğŸ”Œ **å¯æ‰©å±•æ¶æ„**: æ˜“äºæ·»åŠ æ–°çš„é‡åŒ–ç®—æ³•å’Œæ¨¡å‹æ”¯æŒ
- ğŸ–¥ï¸ **å‘½ä»¤è¡Œå·¥å…·**: ç®€å•æ˜“ç”¨çš„CLIæ¥å£
- âš¡ **vLLMå…¼å®¹**: é‡åŒ–çš„æ¨¡å‹å¯ç›´æ¥åœ¨vLLMä¸Šéƒ¨ç½²

## å®‰è£…

```bash
pip install -r requirements.txt
```

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬é‡åŒ–

```bash
python quantize.py --model-path /path/to/model --output-path /path/to/output --method gptq
```

### MinMaxé‡åŒ–

```bash
python quantize.py --model-path /path/to/model --output-path /path/to/output --method minmax --bits 8
```

### é€å±‚é‡åŒ–

```bash
python quantize.py --model-path /path/to/model --output-path /path/to/output --method awq --layer-wise
```

### ä½¿ç”¨ç¦»ç¾¤å€¼æŠ‘åˆ¶

```bash
python quantize.py --model-path /path/to/model --output-path /path/to/output --method gptq --outlier-suppression smooth_quant
```

## vLLMéƒ¨ç½²

é‡åŒ–çš„æ¨¡å‹å¯ä»¥ç›´æ¥åœ¨vLLMä¸Šéƒ¨ç½²ï¼Œæ— éœ€é¢å¤–è½¬æ¢ï¼š

### 1. å®‰è£…vLLM

```bash
pip install vllm
```

### 2. å¯åŠ¨vLLMæœåŠ¡

```bash
# GPTQé‡åŒ–æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/quantized/model \
    --quantization gptq

# AWQé‡åŒ–æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/quantized/model \
    --quantization awq

# MinMaxé‡åŒ–æ¨¡å‹
python -m vllm.entrypoints.openai.api_server \
    --model /path/to/quantized/model \
    --quantization minmax
```

### 3. ä½¿ç”¨API

```python
from vllm import LLM, SamplingParams

# åŠ è½½é‡åŒ–æ¨¡å‹
llm = LLM(model="/path/to/quantized/model", quantization="gptq")

# ç”Ÿæˆæ–‡æœ¬
sampling_params = SamplingParams(temperature=0.7, top_p=0.95)
outputs = llm.generate("Hello, how are you?", sampling_params)
print(outputs[0].outputs[0].text)
```

## æ”¯æŒçš„æ¨¡å‹

- Llama ç³»åˆ— (Llama-2, Llama-3)
- Qwen ç³»åˆ— (Qwen-1.5, Qwen-2)
- DeepSeek ç³»åˆ—
- å…¶ä»–å…¼å®¹ HuggingFace Transformers çš„æ¨¡å‹

## æ”¯æŒçš„é‡åŒ–ç®—æ³•

- **GPTQ**: åŸºäºHessiançŸ©é˜µçš„é‡åŒ–æ–¹æ³•ï¼ŒvLLMåŸç”Ÿæ”¯æŒ
- **AWQ**: Activation-aware Weight Quantizationï¼ŒvLLMåŸç”Ÿæ”¯æŒ
- **MinMax**: åŸºç¡€çš„çº¿æ€§é‡åŒ–æ–¹æ³•ï¼Œæ”¯æŒå¯¹ç§°/éå¯¹ç§°é‡åŒ–ï¼ŒvLLMå…¼å®¹
- **HQQ**: Half-Quadratic Quantization
- **æ›´å¤šç®—æ³•å¯é€šè¿‡æ’ä»¶æ‰©å±•**

## æ”¯æŒçš„ç¦»ç¾¤å€¼æŠ‘åˆ¶ç®—æ³•

- **SmoothQuant**: å¹³æ»‘é‡åŒ–ç®—æ³•
- **æ›´å¤šç®—æ³•å¯é€šè¿‡æ’ä»¶æ‰©å±•**

## é…ç½®ç¤ºä¾‹

æŸ¥çœ‹ `configs/` ç›®å½•ä¸‹çš„é…ç½®æ–‡ä»¶ç¤ºä¾‹ã€‚

## æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„é‡åŒ–ç®—æ³•

1. åœ¨ `quantizers/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„é‡åŒ–å™¨ç±»
2. ç»§æ‰¿ `BaseQuantizer` ç±»
3. å®ç° `quantize()` æ–¹æ³•
4. æ·»åŠ vLLMå…¼å®¹æ€§æ”¯æŒ
5. åœ¨ `quantizer_registry.py` ä¸­æ³¨å†Œ

### æ·»åŠ æ–°çš„æ¨¡å‹æ”¯æŒ

1. åœ¨ `models/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„æ¨¡å‹é€‚é…å™¨
2. ç»§æ‰¿ `BaseModelAdapter` ç±»
3. å®ç°å¿…è¦çš„æ–¹æ³•
4. åœ¨ `model_registry.py` ä¸­æ³¨å†Œ

## è®¸å¯è¯

MIT License 